---
layout: post
title: Week 3
---

I met with Professor Verma on May 25 to continue discussing the existing methodologies for metric learning and other relevant topics. This week we covered Siamese Neural Network (survey by Li et al., 2022) and Hierarchical Similarity Metrics (Verma et al., 2012). SNN is a pair of parallel neural nets that share the same structure and weights, and is used to learn similarity among inputs. HSM uses a nearest-neighbor framework to learn hierarchical metrics that reflect the known taxonomy structure, and provides improved classification accuracy and correct placement of unseen cateogories.

I built a basic SNN in Pytorch using the following toy data sets in 2 dimensions with binary classification: one with two Gaussian distributions, and one with concentric donut rings. The network consists of only linear layers and ReLU activation functions, and I was able to reduce training error within an epoch, but could not produce a model with zero training error for the ringed data (even though the two classes are clearly separated). Will explore further if I need to use an SNN in the future.
